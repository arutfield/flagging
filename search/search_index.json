{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Note This project is still currently under development. If you are interested in joining our team and contributing, read our project wiki for more info. Welcome to the CRWA Flagging Website Documentation! This site provides developers and maintainers information about the CRWA's flagging website, including information on: deploying the website, developing the website, using the website's code locally, and using the website's admin panel. For Website Administrators If the website is already deployed and you want to implement a manual override, you do not need to follow the setup guide. All you need to do is read the admin guide to manage the website while it's deployed. Connecting to Weebly Work in progress For Developers Start by following the setup guide . Once you have the website setup locally, you now have access to the following: Deploy the website to Heroku (guide here ) Manually run commands and download data through the shell . Make changes to the predictive model, including revising its coefficients. (Guide is currently WIP) (Advanced) Make other changes to the website.","title":"Home"},{"location":"#for-website-administrators","text":"If the website is already deployed and you want to implement a manual override, you do not need to follow the setup guide. All you need to do is read the admin guide to manage the website while it's deployed.","title":"For Website Administrators"},{"location":"#connecting-to-weebly","text":"Work in progress","title":"Connecting to Weebly"},{"location":"#for-developers","text":"Start by following the setup guide . Once you have the website setup locally, you now have access to the following: Deploy the website to Heroku (guide here ) Manually run commands and download data through the shell . Make changes to the predictive model, including revising its coefficients. (Guide is currently WIP) (Advanced) Make other changes to the website.","title":"For Developers"},{"location":"about/","text":"About Flagging Website Of the many services that the CRWA provides to the greater Boston community, one of those is monitoring whether it is safe to swim and/or boat in the Charles River. The CRWA Flagging Program uses a system of color-coded flags to indicate whether or not the river's water quality is safe for boating at various boating locations between Watertown and Boston. Flag colors are based on E. coli and cyanobacteria (blue-green algae) levels; blue flags indicate suitable boating conditions and red flags indicate potential health risks. See the website's about page for more about the website functionality and how it relates to the flagging program's objectives. See the development resources overview for more information on how this project started and how we came to make the design decisions that you see here today. Code for Boston Code for Boston is the group that built the CRWA's flagging website. You can find a list of individual contributors here Code for Boston is a volunteer Civic Technology meetup. We are part of the Code for America Brigade network , and are made up of developers, designers, data geeks, citizen activists, and many others who use creative technology to solve civic and social problems. Charles River Via the EPA: The Charles River flows 80 miles from Hopkinton, Mass. to Boston Harbor. The Charles River is the most prominent urban river in New England. It is a major source of recreation and a readily-available connection to the natural world for residents of the Boston metropolitan area. The entire Charles River drains rain and melted snow from a watershed area of 310 square miles. Charles River Watershed Association (CRWA) The Charles River Watershed Association (\"CRWA\") was formed in 1965, the same year that Dirty Water peaked at #11 on the Billboard singles chart. Via the CRWA's website: CRWA is one of the country\u2019s oldest watershed organizations and has figured prominently in major cleanup and protection efforts. Since our earliest days of advocacy, we have worked with government officials and citizen groups from 35 Massachusetts watershed towns from Hopkinton to Boston. The EPA also relies on sample data collected by the CRWA to construct its report card.","title":"About"},{"location":"about/#about","text":"","title":"About"},{"location":"about/#flagging-website","text":"Of the many services that the CRWA provides to the greater Boston community, one of those is monitoring whether it is safe to swim and/or boat in the Charles River. The CRWA Flagging Program uses a system of color-coded flags to indicate whether or not the river's water quality is safe for boating at various boating locations between Watertown and Boston. Flag colors are based on E. coli and cyanobacteria (blue-green algae) levels; blue flags indicate suitable boating conditions and red flags indicate potential health risks. See the website's about page for more about the website functionality and how it relates to the flagging program's objectives. See the development resources overview for more information on how this project started and how we came to make the design decisions that you see here today.","title":"Flagging Website"},{"location":"about/#code-for-boston","text":"Code for Boston is the group that built the CRWA's flagging website. You can find a list of individual contributors here Code for Boston is a volunteer Civic Technology meetup. We are part of the Code for America Brigade network , and are made up of developers, designers, data geeks, citizen activists, and many others who use creative technology to solve civic and social problems.","title":"Code for Boston"},{"location":"about/#charles-river","text":"Via the EPA: The Charles River flows 80 miles from Hopkinton, Mass. to Boston Harbor. The Charles River is the most prominent urban river in New England. It is a major source of recreation and a readily-available connection to the natural world for residents of the Boston metropolitan area. The entire Charles River drains rain and melted snow from a watershed area of 310 square miles.","title":"Charles River"},{"location":"about/#charles-river-watershed-association-crwa","text":"The Charles River Watershed Association (\"CRWA\") was formed in 1965, the same year that Dirty Water peaked at #11 on the Billboard singles chart. Via the CRWA's website: CRWA is one of the country\u2019s oldest watershed organizations and has figured prominently in major cleanup and protection efforts. Since our earliest days of advocacy, we have worked with government officials and citizen groups from 35 Massachusetts watershed towns from Hopkinton to Boston. The EPA also relies on sample data collected by the CRWA to construct its report card.","title":"Charles River Watershed Association (CRWA)"},{"location":"admin/","text":"Admin Panel Note This page discusses how to use the admin panel for the website. For how to set up the admin page username and password during deployment, see the Heroku deployment documentation. The admin panel is used to manually override the model outputs during events and advisories that would adversely effect the river quality. You can reach the admin panel by going to /admin after the base URL for the flagging website. (You need to it in manually.) You will be asked a username and password, which will be provided to you by the person who deployed the website. Enter the correct credentials to enter the admin panel. Note In \"development\" mode, the default username is admin and the password is password . In production, the environment variables BASIC_AUTH_USERNAME and BASIC_AUTH_PASSWORD are used to set the credentials. Cyanobacteria Overrides There should be a link to this page in the admin navigation bar. On this page, one can add an override for a reach with a start time and end time, and if the current time is between those times then the reach will be marked as unsafe on the main website, regardless of the model data.","title":"Admin"},{"location":"admin/#admin-panel","text":"Note This page discusses how to use the admin panel for the website. For how to set up the admin page username and password during deployment, see the Heroku deployment documentation. The admin panel is used to manually override the model outputs during events and advisories that would adversely effect the river quality. You can reach the admin panel by going to /admin after the base URL for the flagging website. (You need to it in manually.) You will be asked a username and password, which will be provided to you by the person who deployed the website. Enter the correct credentials to enter the admin panel. Note In \"development\" mode, the default username is admin and the password is password . In production, the environment variables BASIC_AUTH_USERNAME and BASIC_AUTH_PASSWORD are used to set the credentials.","title":"Admin Panel"},{"location":"admin/#cyanobacteria-overrides","text":"There should be a link to this page in the admin navigation bar. On this page, one can add an override for a reach with a start time and end time, and if the current time is between those times then the reach will be marked as unsafe on the main website, regardless of the model data.","title":"Cyanobacteria Overrides"},{"location":"setup/","text":"Setup This is a guide on how to do your first-time setup for running the website locally and getting ready to make changes to the code base. If you are a developer, you should follow this guide before doing anything else! This guide assumes you're a complete beginner at Python, Git, Postgres, etc. so don't be intimidated if you don't know what all of these things are. The main thing this guide assumes is that you know how to open up a terminal in your respective operating system (command prompt or \"CMD\" in Windows, and bash in OSX). Dependencies Install all of the following programs onto your computer: Required: Python 3 - specifically 3.7 or higher Git (first time setup guide here ) Postgres (see installation instructions below) 7zip (If on OSX, install via Homebrew: brew install p7zip ) (OSX only) Homebrew Recommended: A good text editor or IDE, such as Atom.io (which is lightweight and beginner friendly) or PyCharm (which is powerful but bulky and geared toward advanced users). Heroku CLI (required for remote deployment to Heroku.) Other: It is strongly recommend that you create a GitHub account if you haven't done so already. The GitHub account should have the same email as the one registered to your git config --global user.email that you set in the first time git setup. Warning (Windows users only) At least two Windows users have had problems getting Python working in Windows for the first time. Check out some people troubleshooting various Python installation related issues on StackOverflow . Also note that the command to run Python in Windows may be python , python3 , py , or py3 . Figure out which one works for you. Postgres installation Windows (CMD) Download here and install via the executable. (If you had any terminals open, close out and reopen after Postgres installation.) Open command prompt and try the following (case-sensitive): psql -V If it returns the version number then you're set. If you get an error about the command not being recognized, then it might mean you need to manually add Postgres's bin to your PATH ( see here ). OSX (Bash) If you do not have Homebrew installed, install it from here . Via a bash terminal: brew install postgres Test that it works by running (case-sensitive): psql -V . If it returns the version number then you're set. Tip Chances are you are not going to need Postgres to run in the background constantly, so you should learn how to turn it off and back on. Windows (CMD) Turn Postgres on/off: Go to the Start menu and open up \"Run...\" services.msc -> Enter . This opens the Services panel. Look for the name postgresql and start/stop Postgres. Keep Postgres from running at startup: (Via the Services panel) As long as the service is \"manual\" and not automatic, it will not load at startup. OSX (Bash) Turn Postgres on: pg_ctl -D /usr/local/var/postgres start Turn Postgres off: pg_ctl -D /usr/local/var/postgres stop Keep Postgres from running at startup: Some solutions here . Download and Setup the Code Base The flagging website is open source; the whole website's source code is available on GitHub. This section of the setup guide shows you the preferred way to install it and set up the code on a local computer. Fork the main GitHub repo to your personal GitHub account. You can do that by going to the Code For Boston flagging repo and clicking on this button: Point your terminal (Bash on OSX, command prompt on Windows) to the directory you want to put the /flagging project folder inside of. E.g. if you want the project folder to be located at <yourname>/Documents/flagging , then point your directory to <yourname>/Documents . You can change directories using the cd command: cd \"path/goes/here\" Run the following to download your fork and setup the connection to the upstream remote. Replace YOUR_USERNAME_HERE (in the first line) with your actual GitHub username. git clone https://github.com/YOUR_USERNAME_HERE/flagging/ cd flagging git remote add upstream https://github.com/codeforboston/flagging.git git fetch upstream In your newly created flagging folder, create a file called .env and add the vault password to it. (Replace vault_password_here with the actual vault password if you have it; otherwise just copy+paste and run the command as-is): echo \"VAULT_PASSWORD=vault_password_here\" > .env Danger If you do any commits to the repo, please make sure .env is properly gitignored! ( .flaskenv does not need to be gitignored, only .env .) The vault password is sensitive information; it should not be shared with others and it should not be posted online publicly. Run the Website Locally Note From here on in the documentation, each terminal command assumes your terminal's working directory is pointed toward the flagging directory. After you get everything set up, you should run the website at least once. Te process of running the website installs the remaining dependencies, and sets up a virtual environment to work in. Run the following: Windows (CMD) run_windows_dev OSX (Bash) sh run_unix_dev.sh Note The script being run is doing the following, in order: Set up a \"virtual environment\" (basically an isolated folder inside your project directory that we install the Python packages into), install the packages inside of requirements.txt ; this can take a while during your first time. Set up some environment variables that Flask needs. Prompts the user to set some options for the deployment. (See step 2 below.) Set up the Postgres database and update it with data. Run the actual website. For the first prompt, type y to run the website in offline mode. For the subsequent two prompts, Enter through them without inputting anything. Success You should be good if you eventually see something like the following in your terminal: * Serving Flask app \"flagging_site:create_app\" (lazy loading) * Environment: development * Debug mode: on * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit) * Restarting with stat Point your browser of choice to the URL shown in the terminal output. If everything worked out, the website should be running on your local computer!","title":"Setup"},{"location":"setup/#setup","text":"This is a guide on how to do your first-time setup for running the website locally and getting ready to make changes to the code base. If you are a developer, you should follow this guide before doing anything else! This guide assumes you're a complete beginner at Python, Git, Postgres, etc. so don't be intimidated if you don't know what all of these things are. The main thing this guide assumes is that you know how to open up a terminal in your respective operating system (command prompt or \"CMD\" in Windows, and bash in OSX).","title":"Setup"},{"location":"setup/#dependencies","text":"Install all of the following programs onto your computer: Required: Python 3 - specifically 3.7 or higher Git (first time setup guide here ) Postgres (see installation instructions below) 7zip (If on OSX, install via Homebrew: brew install p7zip ) (OSX only) Homebrew Recommended: A good text editor or IDE, such as Atom.io (which is lightweight and beginner friendly) or PyCharm (which is powerful but bulky and geared toward advanced users). Heroku CLI (required for remote deployment to Heroku.) Other: It is strongly recommend that you create a GitHub account if you haven't done so already. The GitHub account should have the same email as the one registered to your git config --global user.email that you set in the first time git setup. Warning (Windows users only) At least two Windows users have had problems getting Python working in Windows for the first time. Check out some people troubleshooting various Python installation related issues on StackOverflow . Also note that the command to run Python in Windows may be python , python3 , py , or py3 . Figure out which one works for you.","title":"Dependencies"},{"location":"setup/#postgres-installation","text":"Windows (CMD) Download here and install via the executable. (If you had any terminals open, close out and reopen after Postgres installation.) Open command prompt and try the following (case-sensitive): psql -V If it returns the version number then you're set. If you get an error about the command not being recognized, then it might mean you need to manually add Postgres's bin to your PATH ( see here ). OSX (Bash) If you do not have Homebrew installed, install it from here . Via a bash terminal: brew install postgres Test that it works by running (case-sensitive): psql -V . If it returns the version number then you're set. Tip Chances are you are not going to need Postgres to run in the background constantly, so you should learn how to turn it off and back on. Windows (CMD) Turn Postgres on/off: Go to the Start menu and open up \"Run...\" services.msc -> Enter . This opens the Services panel. Look for the name postgresql and start/stop Postgres. Keep Postgres from running at startup: (Via the Services panel) As long as the service is \"manual\" and not automatic, it will not load at startup. OSX (Bash) Turn Postgres on: pg_ctl -D /usr/local/var/postgres start Turn Postgres off: pg_ctl -D /usr/local/var/postgres stop Keep Postgres from running at startup: Some solutions here .","title":"Postgres installation"},{"location":"setup/#download-and-setup-the-code-base","text":"The flagging website is open source; the whole website's source code is available on GitHub. This section of the setup guide shows you the preferred way to install it and set up the code on a local computer. Fork the main GitHub repo to your personal GitHub account. You can do that by going to the Code For Boston flagging repo and clicking on this button: Point your terminal (Bash on OSX, command prompt on Windows) to the directory you want to put the /flagging project folder inside of. E.g. if you want the project folder to be located at <yourname>/Documents/flagging , then point your directory to <yourname>/Documents . You can change directories using the cd command: cd \"path/goes/here\" Run the following to download your fork and setup the connection to the upstream remote. Replace YOUR_USERNAME_HERE (in the first line) with your actual GitHub username. git clone https://github.com/YOUR_USERNAME_HERE/flagging/ cd flagging git remote add upstream https://github.com/codeforboston/flagging.git git fetch upstream In your newly created flagging folder, create a file called .env and add the vault password to it. (Replace vault_password_here with the actual vault password if you have it; otherwise just copy+paste and run the command as-is): echo \"VAULT_PASSWORD=vault_password_here\" > .env Danger If you do any commits to the repo, please make sure .env is properly gitignored! ( .flaskenv does not need to be gitignored, only .env .) The vault password is sensitive information; it should not be shared with others and it should not be posted online publicly.","title":"Download and Setup the Code Base"},{"location":"setup/#run-the-website-locally","text":"Note From here on in the documentation, each terminal command assumes your terminal's working directory is pointed toward the flagging directory. After you get everything set up, you should run the website at least once. Te process of running the website installs the remaining dependencies, and sets up a virtual environment to work in. Run the following: Windows (CMD) run_windows_dev OSX (Bash) sh run_unix_dev.sh Note The script being run is doing the following, in order: Set up a \"virtual environment\" (basically an isolated folder inside your project directory that we install the Python packages into), install the packages inside of requirements.txt ; this can take a while during your first time. Set up some environment variables that Flask needs. Prompts the user to set some options for the deployment. (See step 2 below.) Set up the Postgres database and update it with data. Run the actual website. For the first prompt, type y to run the website in offline mode. For the subsequent two prompts, Enter through them without inputting anything. Success You should be good if you eventually see something like the following in your terminal: * Serving Flask app \"flagging_site:create_app\" (lazy loading) * Environment: development * Debug mode: on * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit) * Restarting with stat Point your browser of choice to the URL shown in the terminal output. If everything worked out, the website should be running on your local computer!","title":"Run the Website Locally"},{"location":"cloud/","text":"Overview The flagging website is designed to be hosted on Heroku . The guide for how to set up deployment is available here . The full cloud deployment depends not only on Heroku but also Twitter's development API. The Twitter bot only needs to be set up once and, notwithstanding exigent circumstances (losing the API key, migrating the bot, or handling a Twitter ban), the Twitter bot does not need any additional maintenance. Nevertheless, there is documentation for how to set up the Twitter bot here .","title":"Overview"},{"location":"cloud/#overview","text":"The flagging website is designed to be hosted on Heroku . The guide for how to set up deployment is available here . The full cloud deployment depends not only on Heroku but also Twitter's development API. The Twitter bot only needs to be set up once and, notwithstanding exigent circumstances (losing the API key, migrating the bot, or handling a Twitter ban), the Twitter bot does not need any additional maintenance. Nevertheless, there is documentation for how to set up the Twitter bot here .","title":"Overview"},{"location":"cloud/heroku_deployment/","text":"Remote Deployment Note This guide is an instruction manual on how to deploy the flagging website to the internet via Heroku. If you just want to run the website locally, you do not need Heroku. Instead, check out the development guide. The following tools are required to deploy the website: Heroku CLI Git First Time Deployment Note In this section, the project name is assumed to be crwa-flagging . If you are deploying to another URL, such as crwa-flagging-staging or another personal domain, then replace each reference to crwa-flagging with that. If you've never deployed the app from your computer, follow these instructions. If you have not already done so, pull the repository to your computer, and then change your directory to it: git clone https://github.com/codeforboston/flagging.git cd ./flagging Additionally, make sure the VAULT_PASSWORD environment variable is set if it has not already been: Windows (CMD) set VAULT_PASSWORD = replace_me_with_pw OSX (Bash) export VAULT_PASSWORD = replace_me_with_pw Login to Heroku, and add Heroku as a remote repo using Heroku's CLI: heroku login heroku git:remote -a crwa-flagging Add the vault password as an environment variable to Heroku. Windows (CMD) heroku config:set VAULT_PASSWORD = %VAULT_PASSWORD% OSX (Bash) heroku config:set VAULT_PASSWORD = ${ VAULT_PASSWORD } Now deploy the app! git push heroku master Now try the following: heroku logs --tail If everything worked out, you should see the following at or near the bottom of the log: 2020-06-13T23:17:54.000000+00:00 app[api]: Build succeeded Note If you see instead see something like [...] State changed from starting to crashed , then read the rest of the output to see what happened. The most common error when deploying to production will be a RuntimeError: Unable to load the vault; bad password provided which is self-explanatory. Update the password, and the website will automatically attempt to redeploy. If you don't see that error, then try to self-diagnose. Go see the website for yourself! You are still not done; you need to do one more step, which is to set up the task scheduler. Subsequent Deployments Heroku doesn't allow you to redeploy the website unless you create a new commit. Add some updates if you need to with git add . then git commit -m \"describe your changes here\" . Note In the very rare case you simply need to redeploy without making any changes to the site, in lieu of the above, simply do git commit --allow-empty -m \"redeploy\" . Once you have done that, Heroku will redeploy the site when you merge your working branch: git push heroku master Tip If you are having any issues here related to merge conflicts, instead of deleting everything and starting over, try to pull the data from the heroku branch in and merge it into your local branch. git fetch heroku git pull heroku master Staging and Production Split It is recommended, though not required, that you have both \"staging\" and \"production\" environments for the website (see here for an explanation), and furthermore it is recommended you deploy to staging and play around with the website to see if it looks right before you ever deploy to production. Managing effectively two separate Heroku apps from a single repository requires a bit of knowledge about how git works. Basically what you're doing is connecting to two separate remote git repositories. The default remote repo is called heroku and it was created by Heroku's CLI. But since you now have two Heroku remote repositories, the Heroku CLI doesn't know what it's supposed to name the 2nd one. So you have to manually name it using git. Run the following command to create a staging environment if it does not already exist. heroku create crwa-flagging-staging Once it exists, add the staging environment as a remote; check to make sure all the remotes look right. The heroku remote should correspond with the production environment, and the staging remote should correspond with the staging environment you just created. git remote add staging https://git.heroku.com/crwa-flagging-staging.git git remote -v Success The above command should output something like this: heroku https://git.heroku.com/crwa-flagging.git ( fetch ) heroku https://git.heroku.com/crwa-flagging.git ( push ) origin https://github.com/YOUR_USERNAME_HERE/flagging.git ( fetch ) origin https://github.com/YOUR_USERNAME_HERE/flagging.git ( push ) staging https://git.heroku.com/crwa-flagging-staging.git ( fetch ) staging https://git.heroku.com/crwa-flagging-staging.git ( push ) upstream https://github.com/codeforboston/flagging.git ( fetch ) upstream https://github.com/codeforboston/flagging.git ( push ) Now all of your heroku commands are going to require specifying the app, but the steps to deploy in staging are otherwise similar to the production deployment: Windows (CMD) heroku config:set --app crwa-flagging-staging VAULT_PASSWORD = %VAULT_PASSWORD% git push staging master heroku logs --app crwa-flagging-staging --tail OSX (Bash) heroku config:set --app crwa-flagging-staging VAULT_PASSWORD = ${ VAULT_PASSWORD } git push staging master heroku logs --app crwa-flagging-staging --tail Check out the website in the staging environment and make sure it looks right.","title":"Heroku Deployment"},{"location":"cloud/heroku_deployment/#remote-deployment","text":"Note This guide is an instruction manual on how to deploy the flagging website to the internet via Heroku. If you just want to run the website locally, you do not need Heroku. Instead, check out the development guide. The following tools are required to deploy the website: Heroku CLI Git","title":"Remote Deployment"},{"location":"cloud/heroku_deployment/#first-time-deployment","text":"Note In this section, the project name is assumed to be crwa-flagging . If you are deploying to another URL, such as crwa-flagging-staging or another personal domain, then replace each reference to crwa-flagging with that. If you've never deployed the app from your computer, follow these instructions. If you have not already done so, pull the repository to your computer, and then change your directory to it: git clone https://github.com/codeforboston/flagging.git cd ./flagging Additionally, make sure the VAULT_PASSWORD environment variable is set if it has not already been: Windows (CMD) set VAULT_PASSWORD = replace_me_with_pw OSX (Bash) export VAULT_PASSWORD = replace_me_with_pw Login to Heroku, and add Heroku as a remote repo using Heroku's CLI: heroku login heroku git:remote -a crwa-flagging Add the vault password as an environment variable to Heroku. Windows (CMD) heroku config:set VAULT_PASSWORD = %VAULT_PASSWORD% OSX (Bash) heroku config:set VAULT_PASSWORD = ${ VAULT_PASSWORD } Now deploy the app! git push heroku master Now try the following: heroku logs --tail If everything worked out, you should see the following at or near the bottom of the log: 2020-06-13T23:17:54.000000+00:00 app[api]: Build succeeded Note If you see instead see something like [...] State changed from starting to crashed , then read the rest of the output to see what happened. The most common error when deploying to production will be a RuntimeError: Unable to load the vault; bad password provided which is self-explanatory. Update the password, and the website will automatically attempt to redeploy. If you don't see that error, then try to self-diagnose. Go see the website for yourself! You are still not done; you need to do one more step, which is to set up the task scheduler.","title":"First Time Deployment"},{"location":"cloud/heroku_deployment/#subsequent-deployments","text":"Heroku doesn't allow you to redeploy the website unless you create a new commit. Add some updates if you need to with git add . then git commit -m \"describe your changes here\" . Note In the very rare case you simply need to redeploy without making any changes to the site, in lieu of the above, simply do git commit --allow-empty -m \"redeploy\" . Once you have done that, Heroku will redeploy the site when you merge your working branch: git push heroku master Tip If you are having any issues here related to merge conflicts, instead of deleting everything and starting over, try to pull the data from the heroku branch in and merge it into your local branch. git fetch heroku git pull heroku master","title":"Subsequent Deployments"},{"location":"cloud/heroku_deployment/#staging-and-production-split","text":"It is recommended, though not required, that you have both \"staging\" and \"production\" environments for the website (see here for an explanation), and furthermore it is recommended you deploy to staging and play around with the website to see if it looks right before you ever deploy to production. Managing effectively two separate Heroku apps from a single repository requires a bit of knowledge about how git works. Basically what you're doing is connecting to two separate remote git repositories. The default remote repo is called heroku and it was created by Heroku's CLI. But since you now have two Heroku remote repositories, the Heroku CLI doesn't know what it's supposed to name the 2nd one. So you have to manually name it using git. Run the following command to create a staging environment if it does not already exist. heroku create crwa-flagging-staging Once it exists, add the staging environment as a remote; check to make sure all the remotes look right. The heroku remote should correspond with the production environment, and the staging remote should correspond with the staging environment you just created. git remote add staging https://git.heroku.com/crwa-flagging-staging.git git remote -v Success The above command should output something like this: heroku https://git.heroku.com/crwa-flagging.git ( fetch ) heroku https://git.heroku.com/crwa-flagging.git ( push ) origin https://github.com/YOUR_USERNAME_HERE/flagging.git ( fetch ) origin https://github.com/YOUR_USERNAME_HERE/flagging.git ( push ) staging https://git.heroku.com/crwa-flagging-staging.git ( fetch ) staging https://git.heroku.com/crwa-flagging-staging.git ( push ) upstream https://github.com/codeforboston/flagging.git ( fetch ) upstream https://github.com/codeforboston/flagging.git ( push ) Now all of your heroku commands are going to require specifying the app, but the steps to deploy in staging are otherwise similar to the production deployment: Windows (CMD) heroku config:set --app crwa-flagging-staging VAULT_PASSWORD = %VAULT_PASSWORD% git push staging master heroku logs --app crwa-flagging-staging --tail OSX (Bash) heroku config:set --app crwa-flagging-staging VAULT_PASSWORD = ${ VAULT_PASSWORD } git push staging master heroku logs --app crwa-flagging-staging --tail Check out the website in the staging environment and make sure it looks right.","title":"Staging and Production Split"},{"location":"cloud/twitter_bot/","text":"Twitter Bot Every time the website updates, it sends out a tweet. In order for it to do that though, you need to set up a Twitter account. First Time Setup Follow these steps to set up the Twitter bot for the first time, such as on a new Twitter account. Create a Twitter account that will host the bot, or login to an account you already have that you want to send automated tweets from. Go to https://apps.twitter.com/ and sign up for a development account. Note that you will need both a valid phone number and a valid email tied to the developer account in order to use development features. Note You will have to wait an hour or two for Twitter.com to get back to you and approve your developer account. Once you are approved, go to the Twitter Developer Portal . Click on the app you created, and in the Settings tab, ensure that the App permissions are set to Read and Write instead of only Read . Tip If at some point during step 3 Twitter starts throwing API keys at you, ignore it for now. We'll get all the keys we need in next couple steps. In the code base, use the VAULT_PASSWORD to unzip the vault.7z manually. You should have a file called secrets.json . Open up secrets.json in the plaintext editor of your choosing. Danger Make sure that you delete the unencrypted, unarchived version of the secrets.json file after you are done with it. Now go back to your browser with the Twitter Developer Portal. At the top of the screen, flip to the Keys and tokens . Now it's time to go through the dashboard and get your copy+paste ready. We will be inserting these values into the secrets.json (remember to wrap the keys in double quotes \"like this\" when you insert them). The API Key & Secret should should go in the corresponding fields for \"api_key\" : \"...\" and \"api_key_secret\" : \"...\" . The Bearer Token should go in the field \"bearer_token\" : \"...\" . The Access Token & Secret should go in the corresponding fields for \"access_token\" : \"...\" and \"access_token_secret\" : \"...\" . But first, you will need to regenerate the Access Token & Secret so that it has both read and write permissions. Success The secrets.json file should look something like this, with the ellipses replacing the actual values: { \"SECRET_KEY\" : \"...\" , \"HOBOLINK_AUTH\" : { \"password\" : \"...\" , \"user\" : \"...\" , \"token\" : \"...\" }, \"TWITTER_AUTH\" : { \"api_key\" : \"...\" , \"api_key_secret\" : \"...\" , \"bearer_token\" : \"...\" , \"access_token\" : \"...\" , \"access_token_secret\" : \"...\" } } Rezip the file. Enter the VAULT_PASSWORD when prompted (happens twice). cd flagging_site 7z a -p vault.7z secrets.json cd .. Delete delete the unencrpyted, unarchived version of the secrets.json file.","title":"Twitter Bot"},{"location":"cloud/twitter_bot/#twitter-bot","text":"Every time the website updates, it sends out a tweet. In order for it to do that though, you need to set up a Twitter account.","title":"Twitter Bot"},{"location":"cloud/twitter_bot/#first-time-setup","text":"Follow these steps to set up the Twitter bot for the first time, such as on a new Twitter account. Create a Twitter account that will host the bot, or login to an account you already have that you want to send automated tweets from. Go to https://apps.twitter.com/ and sign up for a development account. Note that you will need both a valid phone number and a valid email tied to the developer account in order to use development features. Note You will have to wait an hour or two for Twitter.com to get back to you and approve your developer account. Once you are approved, go to the Twitter Developer Portal . Click on the app you created, and in the Settings tab, ensure that the App permissions are set to Read and Write instead of only Read . Tip If at some point during step 3 Twitter starts throwing API keys at you, ignore it for now. We'll get all the keys we need in next couple steps. In the code base, use the VAULT_PASSWORD to unzip the vault.7z manually. You should have a file called secrets.json . Open up secrets.json in the plaintext editor of your choosing. Danger Make sure that you delete the unencrypted, unarchived version of the secrets.json file after you are done with it. Now go back to your browser with the Twitter Developer Portal. At the top of the screen, flip to the Keys and tokens . Now it's time to go through the dashboard and get your copy+paste ready. We will be inserting these values into the secrets.json (remember to wrap the keys in double quotes \"like this\" when you insert them). The API Key & Secret should should go in the corresponding fields for \"api_key\" : \"...\" and \"api_key_secret\" : \"...\" . The Bearer Token should go in the field \"bearer_token\" : \"...\" . The Access Token & Secret should go in the corresponding fields for \"access_token\" : \"...\" and \"access_token_secret\" : \"...\" . But first, you will need to regenerate the Access Token & Secret so that it has both read and write permissions. Success The secrets.json file should look something like this, with the ellipses replacing the actual values: { \"SECRET_KEY\" : \"...\" , \"HOBOLINK_AUTH\" : { \"password\" : \"...\" , \"user\" : \"...\" , \"token\" : \"...\" }, \"TWITTER_AUTH\" : { \"api_key\" : \"...\" , \"api_key_secret\" : \"...\" , \"bearer_token\" : \"...\" , \"access_token\" : \"...\" , \"access_token_secret\" : \"...\" } } Rezip the file. Enter the VAULT_PASSWORD when prompted (happens twice). cd flagging_site 7z a -p vault.7z secrets.json cd .. Delete delete the unencrpyted, unarchived version of the secrets.json file.","title":"First Time Setup"},{"location":"development/","text":"Development - Overview The Development guide is aimed at users who wish to understand the code base and make changes to it if need be. This overview page describes at a high-level what the website's infrastructure is, how it all relates, and why those things are in the app. Tip Make sure to go through the setup guide before doing anything in the development guide. Dependency Diagram classDiagram Heroku <.. gunicorn gunicorn <.. Flask : create_app() gunicorn : /../Procfile Heroku <.. PostgreSQL class Flask Flask : /app.py Flask : create_app() Flask : app = Flask(...) class Config Config : /config.py Config : config = get_config_from_env(...) class vault vault : /vault.7z vault : /app.py Config <.. vault : update_config_from_vault(app) class Swagger Swagger : /app.py Swagger : Swagger(app, ...) Flask <.. Swagger : init_swagger(app) Swagger ..> blueprints : wraps RESTful API Flask <.. Config : app.config.from_object(config) class SQLAlchemy SQLAlchemy : /data/database.py SQLAlchemy : db = SqlAlchemy() class Jinja2 Jinja2 : /app.py Flask <.. Jinja2 : Built-in Flask SQLAlchemy <.. PostgreSQL: Connected via psycopg2 Flask <.. SQLAlchemy : db.init_app(app) class blueprints blueprints : blueprints/flagging.py blueprints : blueprints/api.py blueprints : app.register_blueprint(...) Flask <.. blueprints Jinja2 <.. blueprints : Renders HTML class Admin Admin : /admin.py Admin: admin = Admin(...) SQLAlchemy <.. Admin Flask <.. Admin : init_admin(app) class BasicAuth BasicAuth : /admin.py BasicAuth : auth = BasicAuth() BasicAuth ..> Admin","title":"Overview"},{"location":"development/#development-overview","text":"The Development guide is aimed at users who wish to understand the code base and make changes to it if need be. This overview page describes at a high-level what the website's infrastructure is, how it all relates, and why those things are in the app. Tip Make sure to go through the setup guide before doing anything in the development guide.","title":"Development - Overview"},{"location":"development/#dependency-diagram","text":"classDiagram Heroku <.. gunicorn gunicorn <.. Flask : create_app() gunicorn : /../Procfile Heroku <.. PostgreSQL class Flask Flask : /app.py Flask : create_app() Flask : app = Flask(...) class Config Config : /config.py Config : config = get_config_from_env(...) class vault vault : /vault.7z vault : /app.py Config <.. vault : update_config_from_vault(app) class Swagger Swagger : /app.py Swagger : Swagger(app, ...) Flask <.. Swagger : init_swagger(app) Swagger ..> blueprints : wraps RESTful API Flask <.. Config : app.config.from_object(config) class SQLAlchemy SQLAlchemy : /data/database.py SQLAlchemy : db = SqlAlchemy() class Jinja2 Jinja2 : /app.py Flask <.. Jinja2 : Built-in Flask SQLAlchemy <.. PostgreSQL: Connected via psycopg2 Flask <.. SQLAlchemy : db.init_app(app) class blueprints blueprints : blueprints/flagging.py blueprints : blueprints/api.py blueprints : app.register_blueprint(...) Flask <.. blueprints Jinja2 <.. blueprints : Renders HTML class Admin Admin : /admin.py Admin: admin = Admin(...) SQLAlchemy <.. Admin Flask <.. Admin : init_admin(app) class BasicAuth BasicAuth : /admin.py BasicAuth : auth = BasicAuth() BasicAuth ..> Admin","title":"Dependency Diagram"},{"location":"development/data/","text":"Data High-Level Overview Here is a \"TLDR\" of the data engineering for this website: To get data, we ping two different APIs, combine the responses from those API requests, do some processing and feature engineering of the data, and then run a predictive model on the processed data. To store the data and then later retrieve it for the front-end of the website, we use PostgreSQL database. To actually run the functionality that gets data, processes it, and stores it. we run a scheduled job that runs the command flask update-db at a set time intervals. Actually setting up the database requires a few additional steps during either remote or local deployment ( flask create-db and flask init-db ), however those steps are covered elsewhere in the docs. The update_database() inside of database.py runs four functions elsewhere in the data folder. This flow chart shows how those functions relate to one another (each block is a function; the arrows represent that the function's output is used as an input in the function being pointed at). graph TD A(get_live_hobolink_data) --> C(process_data) B(get_live_usgs_data) --> C C --> D(all_models) The rest of this document explains in more detail what's happening in these functions individually. Sources There are two sources of data for our website: An API hosted by the USGS National Water Information System API that's hooked up to a Waltham based stream gauge (herein \"USGS\" data); An API for a HOBOlink RX3000 Remote Monitoring Station device stationed on the Charles River (herein \"HOBOlink\"). USGS The code for retrieving and processing the HOBOlink data is in flagging_site/data/usgs.py . The USGS API very is straightforward. It's a very typical REST API that takes \"GET\" requests and return well-formatted json data. Our preprocessing of the USGS API consists of parsing the JSON into a Pandas dataframe. The data returned by the USGS API is in 15 minute increments, and it measures the stream flow (cubic feet per second) of the Charles River out in Waltham. HOBOlink The code for retrieving and processing the HOBOlink data is in flagging_site/data/hobolink.py . The HOBOlink device captures various information about the Charles River at the location it's stationed: Air temperature Water temperature Wind speed Photosynthetically active radiation (i.e. sunlight) Rainfall The HOBOlink data is accessed through a REST API using some credentials stored in the vault.zip file. The data actually returned by the API is a combination of a yaml file with a CSV below it, and we just use the CSV part. We then do the following to preprocess the CSV: We remove all timestamps ending :05 , :15 , :25 , :35 , :45 , and :55 . These only contain battery information, not weather information. The final dataframe returned is ultimately in 10 minute increments. We make the timestamp consistently report eastern standard times. We consolidate duplicative columns. The HOBOlink API has a weird issue where sometimes it splits columns of data with the same name, seemingly at random. This issue causes serious data issues if untreated (at one point, it caused our model to fail to update for a couple days), so our function cleans the data. As you can see from the above, the HOBOlink API is a bit finicky for whatever reason, but we have a good data processing solution for these problems. The HOBOlink data is also notoriously slow to retrieve (regardless of whether you ask for 1 hour of data or multiple weeks of data), which is why we belabored building the database portion of the flagging website out in the first place. The HOBOlink API does not seem to be rate limited or subject to fees that scale with usage. Tip You can manually download the latest raw data from this device here . If you want some preprocessed data that implements the above modifications to the output, there is a better way to get that data explained in the shell guide. Combining the data Additional information related to combining the data and how the models work is in the Predictive Models page. Postgres Database PostgresSQL is a free, open-source database management system, and it's what our website uses to store data. On OSX or Linux: We need to setup postgres database first thus enter into the bash terminals: brew install postgresql brew services start postgresql Explanation: We will need to install postgresql in order to create our database. With postgresql installed, we can start up database locally or in our computer. We use brew from homebrew to install and start postgresql services. To get homebrew, consult with this link: https://brew.sh/ To begin initialize a database, enter into the bash terminal: ```shell script export POSTGRES_PASSWORD= enter_password_here createdb -U enter_username_here flagging psql -U enter_username_here -d flagging -c \"DROP USER IF EXISTS flagging; CREATE USER flagging SUPERUSER PASSWORD '${POSTGRES_PASSWORD}'\" Explanation: Postgres password can be any password you choose. We exported your chosen postgres password into `POSTGRES_PASSWORD`, an environment variable, which is a variable set outside a program and is independent in each session. Next, we created a database called `flagging` using a username/rolename, which needs to be a Superuser or having all accesses of postgres. By default, the Superuser rolename can be `postgres` or the username for you OS. To find out, you can go into psql terminal, which we will explain below, and enter `\\du` to see all usernames. Finally, we add the database `flagging` using the env variable in which we save our password. You can see the results using the postgresql terminal which you can open by entering: psql Below are a couple of helpful commands you can use in the postgresql: \\q --to quit \\c database_name --to connect to database \\d --show what tables in current database \\du --show database users \\dt --show tables of current database To run the website, in the project directory `flagging` enter: ```shell script sh run_unix_dev.sh Running the bash script run_unix_dev.sh found in the flagging folder. Inside the scirpt, it defines environment variables FLASK_APP and FLASK_ENV which we need to find app.py. We also export the user input for offline mode, vault password, and postgres password for validation. Finally we initialize a database with a custom flask command flask init-db and finally run the flask application flask run . Regarding in how flask application connects to postgresql, database.py creates an object db = SQLAlchemy() which we will refer again in app.py to configure the flask application to support postgressql from .data import db db.init_app(app) . (We can import the db object beecause __init__.py make the object available as a global variable) Flask supports creating custom commands init-db for initializing database and update-db for updating database. init-db command calls init_db function from database.py and essentially calls execute_sql() which executes the sql file schema.sql that creates all the tables. Then calls update_database() which fills the database with data from usgs, hobolink, etc. update-db command primarily just udpates the table thus does not create new tables. Note: currently we are creating and deleting the database everytime the bashscript and program runs.","title":"Data"},{"location":"development/data/#data","text":"","title":"Data"},{"location":"development/data/#high-level-overview","text":"Here is a \"TLDR\" of the data engineering for this website: To get data, we ping two different APIs, combine the responses from those API requests, do some processing and feature engineering of the data, and then run a predictive model on the processed data. To store the data and then later retrieve it for the front-end of the website, we use PostgreSQL database. To actually run the functionality that gets data, processes it, and stores it. we run a scheduled job that runs the command flask update-db at a set time intervals. Actually setting up the database requires a few additional steps during either remote or local deployment ( flask create-db and flask init-db ), however those steps are covered elsewhere in the docs. The update_database() inside of database.py runs four functions elsewhere in the data folder. This flow chart shows how those functions relate to one another (each block is a function; the arrows represent that the function's output is used as an input in the function being pointed at). graph TD A(get_live_hobolink_data) --> C(process_data) B(get_live_usgs_data) --> C C --> D(all_models) The rest of this document explains in more detail what's happening in these functions individually.","title":"High-Level Overview"},{"location":"development/data/#sources","text":"There are two sources of data for our website: An API hosted by the USGS National Water Information System API that's hooked up to a Waltham based stream gauge (herein \"USGS\" data); An API for a HOBOlink RX3000 Remote Monitoring Station device stationed on the Charles River (herein \"HOBOlink\").","title":"Sources"},{"location":"development/data/#usgs","text":"The code for retrieving and processing the HOBOlink data is in flagging_site/data/usgs.py . The USGS API very is straightforward. It's a very typical REST API that takes \"GET\" requests and return well-formatted json data. Our preprocessing of the USGS API consists of parsing the JSON into a Pandas dataframe. The data returned by the USGS API is in 15 minute increments, and it measures the stream flow (cubic feet per second) of the Charles River out in Waltham.","title":"USGS"},{"location":"development/data/#hobolink","text":"The code for retrieving and processing the HOBOlink data is in flagging_site/data/hobolink.py . The HOBOlink device captures various information about the Charles River at the location it's stationed: Air temperature Water temperature Wind speed Photosynthetically active radiation (i.e. sunlight) Rainfall The HOBOlink data is accessed through a REST API using some credentials stored in the vault.zip file. The data actually returned by the API is a combination of a yaml file with a CSV below it, and we just use the CSV part. We then do the following to preprocess the CSV: We remove all timestamps ending :05 , :15 , :25 , :35 , :45 , and :55 . These only contain battery information, not weather information. The final dataframe returned is ultimately in 10 minute increments. We make the timestamp consistently report eastern standard times. We consolidate duplicative columns. The HOBOlink API has a weird issue where sometimes it splits columns of data with the same name, seemingly at random. This issue causes serious data issues if untreated (at one point, it caused our model to fail to update for a couple days), so our function cleans the data. As you can see from the above, the HOBOlink API is a bit finicky for whatever reason, but we have a good data processing solution for these problems. The HOBOlink data is also notoriously slow to retrieve (regardless of whether you ask for 1 hour of data or multiple weeks of data), which is why we belabored building the database portion of the flagging website out in the first place. The HOBOlink API does not seem to be rate limited or subject to fees that scale with usage. Tip You can manually download the latest raw data from this device here . If you want some preprocessed data that implements the above modifications to the output, there is a better way to get that data explained in the shell guide.","title":"HOBOlink"},{"location":"development/data/#combining-the-data","text":"Additional information related to combining the data and how the models work is in the Predictive Models page.","title":"Combining the data"},{"location":"development/data/#postgres-database","text":"PostgresSQL is a free, open-source database management system, and it's what our website uses to store data. On OSX or Linux: We need to setup postgres database first thus enter into the bash terminals: brew install postgresql brew services start postgresql Explanation: We will need to install postgresql in order to create our database. With postgresql installed, we can start up database locally or in our computer. We use brew from homebrew to install and start postgresql services. To get homebrew, consult with this link: https://brew.sh/ To begin initialize a database, enter into the bash terminal: ```shell script export POSTGRES_PASSWORD= enter_password_here createdb -U enter_username_here flagging psql -U enter_username_here -d flagging -c \"DROP USER IF EXISTS flagging; CREATE USER flagging SUPERUSER PASSWORD '${POSTGRES_PASSWORD}'\" Explanation: Postgres password can be any password you choose. We exported your chosen postgres password into `POSTGRES_PASSWORD`, an environment variable, which is a variable set outside a program and is independent in each session. Next, we created a database called `flagging` using a username/rolename, which needs to be a Superuser or having all accesses of postgres. By default, the Superuser rolename can be `postgres` or the username for you OS. To find out, you can go into psql terminal, which we will explain below, and enter `\\du` to see all usernames. Finally, we add the database `flagging` using the env variable in which we save our password. You can see the results using the postgresql terminal which you can open by entering: psql Below are a couple of helpful commands you can use in the postgresql: \\q --to quit \\c database_name --to connect to database \\d --show what tables in current database \\du --show database users \\dt --show tables of current database To run the website, in the project directory `flagging` enter: ```shell script sh run_unix_dev.sh Running the bash script run_unix_dev.sh found in the flagging folder. Inside the scirpt, it defines environment variables FLASK_APP and FLASK_ENV which we need to find app.py. We also export the user input for offline mode, vault password, and postgres password for validation. Finally we initialize a database with a custom flask command flask init-db and finally run the flask application flask run . Regarding in how flask application connects to postgresql, database.py creates an object db = SQLAlchemy() which we will refer again in app.py to configure the flask application to support postgressql from .data import db db.init_app(app) . (We can import the db object beecause __init__.py make the object available as a global variable) Flask supports creating custom commands init-db for initializing database and update-db for updating database. init-db command calls init_db function from database.py and essentially calls execute_sql() which executes the sql file schema.sql that creates all the tables. Then calls update_database() which fills the database with data from usgs, hobolink, etc. update-db command primarily just udpates the table thus does not create new tables. Note: currently we are creating and deleting the database everytime the bashscript and program runs.","title":"Postgres Database"},{"location":"development/predictive_models/","text":"Predictive Models The Flagging Website is basically just a deployed predictive model, so in a sense this document covers the real core of the code base. This page explains the models and the data transformations that occur from the original data. At the bottom, there are some notes on how to change the model coefficients and rerun the website with new coefficients. The predictive models are stored in the file /flagging_site/data/models.py . These models are run as part of the update-db command. The input for the models are a combination of the HOBOlink and USGS data with some transformations of the data. The outputs are stored in the SQL table named model_outputs . Tip There is a fair bit of Pandas in this document and it may be intimidating. However, if you only want to change the model's coefficients and nothing more, you won't need to touch the Pandas directly. Data Transformations The model combines data from the HOBOlink device and USGS. These two data sources are run through the function process_data() , in which the data is aggregated to hourly intervals (USGS is every 15 minutes and HOBOlink is every 10 minutes). Once the data sources are aligned, additional feature transformations are performed such as rolling averages, rolling sums, and a measure of when the last significant rainfall was. The feature transformations the CRWA uses depends on the year of the model, so by the time you may be reading this, this information may be outdated. Previous versions included rolling average wind speeds and air/water temperatures over 24 hours. The current version of the model (as of 2020) calculates the following: Rolling 24 hours of the PAR (photosynthetically active radiation) and the stream flow (cubic feet per second). Rolling sum of rainfall over the following intervals: 0-24h, 0-48h, and 24-48h. The numbers of days since the last \"significant rainfall,\" where significant rain is defined as when the rolling sum of the last 24 hours of rainfall is at least 0.20 inches. Tip If you look at the code, you'll see a lot of stuff like rolling ( 24 ) . The reason rolling works is because the dataframe is sorted already by timestamp at that point by df = df . sort_values ( 'time' ) . Note We use 28 days of HOBOlink data to process the model. For most features, we only need the last 48 hours worth of data to calculate the most recent value, however the last significant rainfall feature requires a lot of historic data because it is not technically bounded or transformed otherwise. This means that even when calculating 1 row of output data, i.e. the latest hour of data, we still need 28 days. In the deployed model, if we do not see any significant rainfall in the last 28 days, we return the difference between the timestamp and the earliest time in the dataframe, df [ 'time' ] . min () . In this scenario, the data will no longer be temporally consistent: a calculation right now will have 28.0 for 'days_since_sig_rain' , but 12 hours from now it will be 27.5. This is fine though because the model will basically never predict E. coli blooms with 28+ days since significant rain, even when the data is not censored. Unfortunately there's no pretty way to implement days_since_sig_rain , so the Pandas code that does all of this is one of the more inscrutable parts of the codebase. Note that 'last_sig_rain' is calculating the timestamp of the last significant rain, and 'days_since_sig_rain' calculates the time delta and translates into days: df [ 'sig_rain' ] = df [ 'rain_0_to_24h_sum' ] >= SIGNIFICANT_RAIN df [ 'last_sig_rain' ] = ( df [ 'time' ] . where ( df [ 'sig_rain' ]) . ffill () . fillna ( df [ 'time' ] . min ()) ) df [ 'days_since_sig_rain' ] = ( ( df [ 'time' ] - df [ 'last_sig_rain' ]) . dt . seconds / 60 / 60 / 24 ) Model Overviews Each model function defined in models.py is formatted like this: Take the last few rows of the input dataframe (I discuss what the input dataframe is later on this page). Each row is an hour of data on the condition of the Charles River and its surrounding environment, so for example, taking the last 24 rows is equivalent to taking the last 24 hours of data. Predict the probability of the water being unsafe using a logistic regression fit, with the coefficients in the log odds form (so the dot product of the parameters and the data returns a predicted log odds of the target variable). To get the probability of a log odds, we run it through a logistic function ( sigmoid() , defined at the top of models.py ). We check whether the function is above or below the target threshold for safety, defined by SAFETY_THRESHOLD . Lastly, we return a dataframe with 5 columns of data: 'reach' , 'time' , 'log_odds' (step 2), 'probability' (step 3), and 'safe' (step 4). Each row in output corresponds to a row of input data. Here is an example function. It should be pretty easy to track the steps outlined above with the code below. def reach_3_model ( df : pd . DataFrame , rows : int = 48 ) -> pd . DataFrame : \"\"\" a- rainfall sum 0-24 hrs b- rainfall sum 24-48 hr d- Days since last rain 0.267*a + 0.1681*b - 0.02855*d + 0.5157 Args: df: (pd.DataFrame) Input data from `process_data()` rows: (int) Number of rows to return. Returns: Outputs for model as a dataframe. \"\"\" df = df . tail ( n = rows ) . copy () df [ 'log_odds' ] = ( 0.5157 + 0.267 * df [ 'rain_0_to_24h_sum' ] + 0.1681 * df [ 'rain_24_to_48h_sum' ] - 0.02855 * df [ 'days_since_sig_rain' ] ) df [ 'probability' ] = sigmoid ( df [ 'log_odds' ]) df [ 'safe' ] = df [ 'probability' ] <= SAFETY_THRESHOLD df [ 'reach' ] = 3 return df [[ 'reach' , 'time' , 'log_odds' , 'probability' , 'safe' ]] Editing the Models Note This section covers making changes to the following: The coefficients for the models. The safety threshold. The model features. If you want to do anything more complicated, such as adding a new source of information to the model, that is outside the scope of this document. To accomplish that, you'll need to do more sleuthing into the code to really understand it. Note Making any changes covered in this section is relatively easy, but you'll still need to actually deploy the changes to Heroku if you want them to be on the live site. Read the deployment guide for more. Model coefficients As covered in the last section, each model's coefficients are represented as log odds ratios. Don't be confused by this statement though: this is how logistic regression is represented in all statistical software packages-- Logit in Python's Statsmodels, logit in Stata, and glm in R-- since that's what's being calculated mathematically when a logistic regression is calculated. I only emphasize this to point out that to get a probability, the final log odds needs to be logistically transformed (which is done via the sigmoid() function) after the linear terms are summed up. The code representing the logistic model prediction was organized for maximum legibility: the first number is the constant term, and the remaining coefficients are aligned next to the column name. Note the final coefficient in this particular example is a negative coefficient and is thus subtracted. df [ 'log_odds' ] = ( 0.5157 + 0.267 * df [ 'rain_0_to_24h_sum' ] + 0.1681 * df [ 'rain_24_to_48h_sum' ] - 0.02855 * df [ 'days_since_sig_rain' ] ) Changing the coefficients is as simple as just changing one of those numbers next to its respective column name, inside of its respective model for any particular reach. Safety threshold The safety threshold is defined near the top of the document: SAFETY_THRESHOLD = 0.65 This represents a 65% threshold for whether or not we consider the water safe or not. The SAFETY_THRESHOLD value is just used as a placeholder/convenience for whatever the default threshold should be. You can always change this value to be lower or higher, and additionally you can replace SAFETY_THRESHOLD inside of a model function Warning Hopefully this goes without saying, but if you are going to change the threshold, please have a good, scientifically and statistically justifiable reason for doing so! Feature transformations Feature transformations occur in the process_data() function after the data has been aggregated by hour, merged, and sorted by timestamp. If you want to add some feature transformations, my suggestion is you try to learn from existing examples and copy+paste with the necessary replacements. If you have a feature that can't be built from a copy+paste, that's where you'll possibly need to learn a bit of Pandas.","title":"Predictive Models"},{"location":"development/predictive_models/#predictive-models","text":"The Flagging Website is basically just a deployed predictive model, so in a sense this document covers the real core of the code base. This page explains the models and the data transformations that occur from the original data. At the bottom, there are some notes on how to change the model coefficients and rerun the website with new coefficients. The predictive models are stored in the file /flagging_site/data/models.py . These models are run as part of the update-db command. The input for the models are a combination of the HOBOlink and USGS data with some transformations of the data. The outputs are stored in the SQL table named model_outputs . Tip There is a fair bit of Pandas in this document and it may be intimidating. However, if you only want to change the model's coefficients and nothing more, you won't need to touch the Pandas directly.","title":"Predictive Models"},{"location":"development/predictive_models/#data-transformations","text":"The model combines data from the HOBOlink device and USGS. These two data sources are run through the function process_data() , in which the data is aggregated to hourly intervals (USGS is every 15 minutes and HOBOlink is every 10 minutes). Once the data sources are aligned, additional feature transformations are performed such as rolling averages, rolling sums, and a measure of when the last significant rainfall was. The feature transformations the CRWA uses depends on the year of the model, so by the time you may be reading this, this information may be outdated. Previous versions included rolling average wind speeds and air/water temperatures over 24 hours. The current version of the model (as of 2020) calculates the following: Rolling 24 hours of the PAR (photosynthetically active radiation) and the stream flow (cubic feet per second). Rolling sum of rainfall over the following intervals: 0-24h, 0-48h, and 24-48h. The numbers of days since the last \"significant rainfall,\" where significant rain is defined as when the rolling sum of the last 24 hours of rainfall is at least 0.20 inches. Tip If you look at the code, you'll see a lot of stuff like rolling ( 24 ) . The reason rolling works is because the dataframe is sorted already by timestamp at that point by df = df . sort_values ( 'time' ) . Note We use 28 days of HOBOlink data to process the model. For most features, we only need the last 48 hours worth of data to calculate the most recent value, however the last significant rainfall feature requires a lot of historic data because it is not technically bounded or transformed otherwise. This means that even when calculating 1 row of output data, i.e. the latest hour of data, we still need 28 days. In the deployed model, if we do not see any significant rainfall in the last 28 days, we return the difference between the timestamp and the earliest time in the dataframe, df [ 'time' ] . min () . In this scenario, the data will no longer be temporally consistent: a calculation right now will have 28.0 for 'days_since_sig_rain' , but 12 hours from now it will be 27.5. This is fine though because the model will basically never predict E. coli blooms with 28+ days since significant rain, even when the data is not censored. Unfortunately there's no pretty way to implement days_since_sig_rain , so the Pandas code that does all of this is one of the more inscrutable parts of the codebase. Note that 'last_sig_rain' is calculating the timestamp of the last significant rain, and 'days_since_sig_rain' calculates the time delta and translates into days: df [ 'sig_rain' ] = df [ 'rain_0_to_24h_sum' ] >= SIGNIFICANT_RAIN df [ 'last_sig_rain' ] = ( df [ 'time' ] . where ( df [ 'sig_rain' ]) . ffill () . fillna ( df [ 'time' ] . min ()) ) df [ 'days_since_sig_rain' ] = ( ( df [ 'time' ] - df [ 'last_sig_rain' ]) . dt . seconds / 60 / 60 / 24 )","title":"Data Transformations"},{"location":"development/predictive_models/#model-overviews","text":"Each model function defined in models.py is formatted like this: Take the last few rows of the input dataframe (I discuss what the input dataframe is later on this page). Each row is an hour of data on the condition of the Charles River and its surrounding environment, so for example, taking the last 24 rows is equivalent to taking the last 24 hours of data. Predict the probability of the water being unsafe using a logistic regression fit, with the coefficients in the log odds form (so the dot product of the parameters and the data returns a predicted log odds of the target variable). To get the probability of a log odds, we run it through a logistic function ( sigmoid() , defined at the top of models.py ). We check whether the function is above or below the target threshold for safety, defined by SAFETY_THRESHOLD . Lastly, we return a dataframe with 5 columns of data: 'reach' , 'time' , 'log_odds' (step 2), 'probability' (step 3), and 'safe' (step 4). Each row in output corresponds to a row of input data. Here is an example function. It should be pretty easy to track the steps outlined above with the code below. def reach_3_model ( df : pd . DataFrame , rows : int = 48 ) -> pd . DataFrame : \"\"\" a- rainfall sum 0-24 hrs b- rainfall sum 24-48 hr d- Days since last rain 0.267*a + 0.1681*b - 0.02855*d + 0.5157 Args: df: (pd.DataFrame) Input data from `process_data()` rows: (int) Number of rows to return. Returns: Outputs for model as a dataframe. \"\"\" df = df . tail ( n = rows ) . copy () df [ 'log_odds' ] = ( 0.5157 + 0.267 * df [ 'rain_0_to_24h_sum' ] + 0.1681 * df [ 'rain_24_to_48h_sum' ] - 0.02855 * df [ 'days_since_sig_rain' ] ) df [ 'probability' ] = sigmoid ( df [ 'log_odds' ]) df [ 'safe' ] = df [ 'probability' ] <= SAFETY_THRESHOLD df [ 'reach' ] = 3 return df [[ 'reach' , 'time' , 'log_odds' , 'probability' , 'safe' ]]","title":"Model Overviews"},{"location":"development/predictive_models/#editing-the-models","text":"Note This section covers making changes to the following: The coefficients for the models. The safety threshold. The model features. If you want to do anything more complicated, such as adding a new source of information to the model, that is outside the scope of this document. To accomplish that, you'll need to do more sleuthing into the code to really understand it. Note Making any changes covered in this section is relatively easy, but you'll still need to actually deploy the changes to Heroku if you want them to be on the live site. Read the deployment guide for more.","title":"Editing the Models"},{"location":"development/predictive_models/#model-coefficients","text":"As covered in the last section, each model's coefficients are represented as log odds ratios. Don't be confused by this statement though: this is how logistic regression is represented in all statistical software packages-- Logit in Python's Statsmodels, logit in Stata, and glm in R-- since that's what's being calculated mathematically when a logistic regression is calculated. I only emphasize this to point out that to get a probability, the final log odds needs to be logistically transformed (which is done via the sigmoid() function) after the linear terms are summed up. The code representing the logistic model prediction was organized for maximum legibility: the first number is the constant term, and the remaining coefficients are aligned next to the column name. Note the final coefficient in this particular example is a negative coefficient and is thus subtracted. df [ 'log_odds' ] = ( 0.5157 + 0.267 * df [ 'rain_0_to_24h_sum' ] + 0.1681 * df [ 'rain_24_to_48h_sum' ] - 0.02855 * df [ 'days_since_sig_rain' ] ) Changing the coefficients is as simple as just changing one of those numbers next to its respective column name, inside of its respective model for any particular reach.","title":"Model coefficients"},{"location":"development/predictive_models/#safety-threshold","text":"The safety threshold is defined near the top of the document: SAFETY_THRESHOLD = 0.65 This represents a 65% threshold for whether or not we consider the water safe or not. The SAFETY_THRESHOLD value is just used as a placeholder/convenience for whatever the default threshold should be. You can always change this value to be lower or higher, and additionally you can replace SAFETY_THRESHOLD inside of a model function Warning Hopefully this goes without saying, but if you are going to change the threshold, please have a good, scientifically and statistically justifiable reason for doing so!","title":"Safety threshold"},{"location":"development/predictive_models/#feature-transformations","text":"Feature transformations occur in the process_data() function after the data has been aggregated by hour, merged, and sorted by timestamp. If you want to add some feature transformations, my suggestion is you try to learn from existing examples and copy+paste with the necessary replacements. If you have a feature that can't be built from a copy+paste, that's where you'll possibly need to learn a bit of Pandas.","title":"Feature transformations"},{"location":"development_resources/","text":"Stack Project History Traditionally, the CRWA Flagging Program was hosted on a PHP-built website that hosted a predictive model and ran it. However, that website was out of commission due to some bugs and the CRWA's lack of PHP development resources. We at Code for Boston attempted to fix the website, although we have had trouble maintaining a steady stream of PHP expertise, so we rebuilt the website from scratch in Python. The project's source code is available on GitHub , and the docs we used for project management and some dev stuff are available in the repo's wiki . Why Python? Python proves to be an excellent choice for the development of this website. Due to how the CRWA tends to staff its team (academics and scientists), Python is the most viable language that a website can be built in while still being maintainable by the CRWA. The two most popular coding languages in academia are R and Python. You can't really build a website in R (you technically can, but really really shouldn't for a lot of reasons). So the next best option is Python. Even if the CRWA does not staff people based on their Python knowledge (we do not expect that they will do this), they are very likely have connections to various people who know Python. It is unlikely that the CRWA will have as many direct ties to people who have Javascript or PHP knowledge. Because long-term maintainability is such a high priority, Python is the sensible technical solution. Not only is Python way more popular than PHP in academia, it's the most popular programming language in general . This means that Python is a natural fit for any organization's coding projects that do not have specialized needs for a particular coding language. Why Flask? Once we have decided on Python for web development, we need to make a determination on whether to use Django or Flask, the two leading frameworks for building websites in Python. Django is designed for much more complicated websites than what we would be building. Django has its own idiom that takes a lot of time to learn and get used to. On the other hand, Flask is a very simple and lightweight framework built mainly around the use of its \" app.route() \" decorator. Why Heroku? Heroku's main advantage is that we can run it for free; the CRWA does not want to spend money if they can avoid doing so. One alternative was Google Cloud , specifically the Google App Engine . We did not do this mainly as it is more work to set up for developers and controlling costs requires extra care. E.g. the always free tier of Google Cloud still requires users to plug in a payment method. Developers who want to test Google Cloud functionality would also run into some of those limitations too, depending on their past history with Google Cloud. With that said, Heroku does provide some excellent benefits focused around how lightweight it is. Google Cloud is not shy about the fact that it can host massive enterprise websites with extremely complicated infrastructural needs. Don't get me wrong: Heroku can host large websites too. But Heroku supports small to medium sites extremely well, and it is really nice for open source websites in particular. Heroku is less opinionated about how you manage your website, whereas Google Cloud products tend to push you toward Google's various Python integrations and APIs. Google Cloud is a behemoth of various services that can overwhelm users, whereas Heroku is conceptually easier to understand. Heroku integrates much more nicely into Flask's extensive use of CLIs. For example, Heroku's task scheduler tool (which is very easy to set up) can simply run a command line script built in Flask. Google App Engine lets you do a simple cron job setup that sends GET requests to your app , but doing something that doesn't publicly expose the interface requires use of three additional services : Pub/Sub, Firestore, and Cloud Scheduler. We want to publicly host this website, but we don't want to expose the keys we use for various things. This is a bit easier to do with Heroku, as it has the concept of an environment that lives on the instance's memory and can be set through the CLI. Google App Engine lets you configure the environment only through app.yaml , which is an issue because it means we'd need to gitignore the app.yaml . (We want to just gitignore the keys, not the whole cloud deployment config!) Warning If you ever want to run this website on Google App Engine, you'll have to make some changes to the repository (such as adding an app.yaml ), and it may also involve making changes to the code-- mainly the data backend and the task scheduler interface. Why Pandas? We made the decision to use Pandas to manipulate data in the backend of the website because it has an interface that should feel familiar to users of Stata, R, or other statistical software packages commonly used by scientists and academics. This ultimately helps with the maintainability of the website for its intended audience. Data manipulation in SQL can sometimes be unintuitive and require advanced trickery (CTEs, window functions) compared to Pandas code that achieves the same results. Additionally, SQL code tends to be formatted in a non-chronological way, e.g. subqueries run before the query that references them, but occur somewhere in the middle of a query. This isn't hard if you use SQL a bit, but it's not intuitive until you've done a bit of SQL. It's true that Pandas is not as efficient as SQL, but we're not processing millions of rows of data, we're only processing a few hundred rows at a time and at infrequent intervals. (And even if efficiency was a concern, we'd sooner use something like Dask than SQL.) One possible downside of Pandas compared to SQL is that SQL has been around for a very long time, and is more of a \"standardized\" thing than Pandas is or perhaps ever will be. We went with the choice for Pandas after discussing it with some academic friends, but we are aware that in the non-academic world, there are more people who know SQL than Pandas. We do use SQL in this website, but primarily to store and retrieve data and to access some features that integrate nicely with the SQLAlchemy ORM (notably the Flask-Admin extension).","title":"Overview"},{"location":"development_resources/#stack","text":"","title":"Stack"},{"location":"development_resources/#project-history","text":"Traditionally, the CRWA Flagging Program was hosted on a PHP-built website that hosted a predictive model and ran it. However, that website was out of commission due to some bugs and the CRWA's lack of PHP development resources. We at Code for Boston attempted to fix the website, although we have had trouble maintaining a steady stream of PHP expertise, so we rebuilt the website from scratch in Python. The project's source code is available on GitHub , and the docs we used for project management and some dev stuff are available in the repo's wiki .","title":"Project History"},{"location":"development_resources/#why-python","text":"Python proves to be an excellent choice for the development of this website. Due to how the CRWA tends to staff its team (academics and scientists), Python is the most viable language that a website can be built in while still being maintainable by the CRWA. The two most popular coding languages in academia are R and Python. You can't really build a website in R (you technically can, but really really shouldn't for a lot of reasons). So the next best option is Python. Even if the CRWA does not staff people based on their Python knowledge (we do not expect that they will do this), they are very likely have connections to various people who know Python. It is unlikely that the CRWA will have as many direct ties to people who have Javascript or PHP knowledge. Because long-term maintainability is such a high priority, Python is the sensible technical solution. Not only is Python way more popular than PHP in academia, it's the most popular programming language in general . This means that Python is a natural fit for any organization's coding projects that do not have specialized needs for a particular coding language.","title":"Why Python?"},{"location":"development_resources/#why-flask","text":"Once we have decided on Python for web development, we need to make a determination on whether to use Django or Flask, the two leading frameworks for building websites in Python. Django is designed for much more complicated websites than what we would be building. Django has its own idiom that takes a lot of time to learn and get used to. On the other hand, Flask is a very simple and lightweight framework built mainly around the use of its \" app.route() \" decorator.","title":"Why Flask?"},{"location":"development_resources/#why-heroku","text":"Heroku's main advantage is that we can run it for free; the CRWA does not want to spend money if they can avoid doing so. One alternative was Google Cloud , specifically the Google App Engine . We did not do this mainly as it is more work to set up for developers and controlling costs requires extra care. E.g. the always free tier of Google Cloud still requires users to plug in a payment method. Developers who want to test Google Cloud functionality would also run into some of those limitations too, depending on their past history with Google Cloud. With that said, Heroku does provide some excellent benefits focused around how lightweight it is. Google Cloud is not shy about the fact that it can host massive enterprise websites with extremely complicated infrastructural needs. Don't get me wrong: Heroku can host large websites too. But Heroku supports small to medium sites extremely well, and it is really nice for open source websites in particular. Heroku is less opinionated about how you manage your website, whereas Google Cloud products tend to push you toward Google's various Python integrations and APIs. Google Cloud is a behemoth of various services that can overwhelm users, whereas Heroku is conceptually easier to understand. Heroku integrates much more nicely into Flask's extensive use of CLIs. For example, Heroku's task scheduler tool (which is very easy to set up) can simply run a command line script built in Flask. Google App Engine lets you do a simple cron job setup that sends GET requests to your app , but doing something that doesn't publicly expose the interface requires use of three additional services : Pub/Sub, Firestore, and Cloud Scheduler. We want to publicly host this website, but we don't want to expose the keys we use for various things. This is a bit easier to do with Heroku, as it has the concept of an environment that lives on the instance's memory and can be set through the CLI. Google App Engine lets you configure the environment only through app.yaml , which is an issue because it means we'd need to gitignore the app.yaml . (We want to just gitignore the keys, not the whole cloud deployment config!) Warning If you ever want to run this website on Google App Engine, you'll have to make some changes to the repository (such as adding an app.yaml ), and it may also involve making changes to the code-- mainly the data backend and the task scheduler interface.","title":"Why Heroku?"},{"location":"development_resources/#why-pandas","text":"We made the decision to use Pandas to manipulate data in the backend of the website because it has an interface that should feel familiar to users of Stata, R, or other statistical software packages commonly used by scientists and academics. This ultimately helps with the maintainability of the website for its intended audience. Data manipulation in SQL can sometimes be unintuitive and require advanced trickery (CTEs, window functions) compared to Pandas code that achieves the same results. Additionally, SQL code tends to be formatted in a non-chronological way, e.g. subqueries run before the query that references them, but occur somewhere in the middle of a query. This isn't hard if you use SQL a bit, but it's not intuitive until you've done a bit of SQL. It's true that Pandas is not as efficient as SQL, but we're not processing millions of rows of data, we're only processing a few hundred rows at a time and at infrequent intervals. (And even if efficiency was a concern, we'd sooner use something like Dask than SQL.) One possible downside of Pandas compared to SQL is that SQL has been around for a very long time, and is more of a \"standardized\" thing than Pandas is or perhaps ever will be. We went with the choice for Pandas after discussing it with some academic friends, but we are aware that in the non-academic world, there are more people who know SQL than Pandas. We do use SQL in this website, but primarily to store and retrieve data and to access some features that integrate nicely with the SQLAlchemy ORM (notably the Flask-Admin extension).","title":"Why Pandas?"},{"location":"development_resources/learning_resources/","text":"Learning Resources Tip Unless you want to overhaul the website or do some bugfixing, you probably don't need to learn any of the frameworks here. The Flagging Website documentation is detailed, self-contained, and should cover the vast majority of use cases for the Flagging website from an administrative perspective, such as updating the predictive model and deploying the website to Heroku. The code base is mainly built with the following frameworks; all of these but the last one are Python frameworks: Pandas (data manipulation framework, built on top of another framework called numpy .) Flask (web framework that handles routing of the website) Jinja2 (text markup framework that is used for adding programmatic logic to statically rendered HTML pages.) Click (CLI building framework) Postgresql These frameworks may be intimidating if this is your first time seeing them and you want to make changes to the site. This page has some learning resources that can help you learn these frameworks. Flask & Jinja2 The official Flask tutorial is excellent and worth following if you want to learn both Flask and Jinja2. Tip Our website's code base is organized somewhat similar to the code base built in the official Flask tutorial. If you are confused by how the code base is organized, going through the tutorial may help clarify some of our design choices. For more examples of larger Flask websites, check out the flask-bones template; we did not explicitly reference it in constructing our website but it nevertheless follows a lot of the same ideas we use. Pandas The Pandas documentation has excellent resources for users who are coming from R, Stata, or SAS: https://pandas.pydata.org/docs/getting_started/comparison/comparison_with_r.html Click Click is pretty easy to understand: it lets you wrap your Python functions with decorators to make the code run on the command line. We use Click to do a lot of our database management. The homepage for Click's documentation should give you a good idea of what Click is all about. Additionally, Flask's documentation has a page here that discusses Flask's integration with Click. Postgresql We do not do anything crazy with Postgresql. We made a deliberate decision to only use SQL for retrieving and storing data, and to avoid some of the more intermediate to advanced aspects of Postgres such as CTEs, views, and so on. Actual data manipulation is done in Pandas. A simple intro SQL tutorial should be more than sufficient for understanding the SQL we use in this code base.","title":"Learning Resources"},{"location":"development_resources/learning_resources/#learning-resources","text":"Tip Unless you want to overhaul the website or do some bugfixing, you probably don't need to learn any of the frameworks here. The Flagging Website documentation is detailed, self-contained, and should cover the vast majority of use cases for the Flagging website from an administrative perspective, such as updating the predictive model and deploying the website to Heroku. The code base is mainly built with the following frameworks; all of these but the last one are Python frameworks: Pandas (data manipulation framework, built on top of another framework called numpy .) Flask (web framework that handles routing of the website) Jinja2 (text markup framework that is used for adding programmatic logic to statically rendered HTML pages.) Click (CLI building framework) Postgresql These frameworks may be intimidating if this is your first time seeing them and you want to make changes to the site. This page has some learning resources that can help you learn these frameworks.","title":"Learning Resources"},{"location":"development_resources/learning_resources/#flask-jinja2","text":"The official Flask tutorial is excellent and worth following if you want to learn both Flask and Jinja2. Tip Our website's code base is organized somewhat similar to the code base built in the official Flask tutorial. If you are confused by how the code base is organized, going through the tutorial may help clarify some of our design choices. For more examples of larger Flask websites, check out the flask-bones template; we did not explicitly reference it in constructing our website but it nevertheless follows a lot of the same ideas we use.","title":"Flask &amp; Jinja2"},{"location":"development_resources/learning_resources/#pandas","text":"The Pandas documentation has excellent resources for users who are coming from R, Stata, or SAS: https://pandas.pydata.org/docs/getting_started/comparison/comparison_with_r.html","title":"Pandas"},{"location":"development_resources/learning_resources/#click","text":"Click is pretty easy to understand: it lets you wrap your Python functions with decorators to make the code run on the command line. We use Click to do a lot of our database management. The homepage for Click's documentation should give you a good idea of what Click is all about. Additionally, Flask's documentation has a page here that discusses Flask's integration with Click.","title":"Click"},{"location":"development_resources/learning_resources/#postgresql","text":"We do not do anything crazy with Postgresql. We made a deliberate decision to only use SQL for retrieving and storing data, and to avoid some of the more intermediate to advanced aspects of Postgres such as CTEs, views, and so on. Actual data manipulation is done in Pandas. A simple intro SQL tutorial should be more than sufficient for understanding the SQL we use in this code base.","title":"Postgresql"},{"location":"development_resources/shell/","text":"Flask Shell Documentation The shell is used to access app functions and data, such as Hobolink and USGS data and access to the database. The reason why the shell is useful is because there may be cases where you want to play around with the app's functions. For example, maybe you see something that seems fishy in the data, so you want to have direct access to the function the website is running. You may also want to The way Flask works makes it impossible to run the website's functions outside the Flask app context, which means importing the functions into a naked shell doesn't work as intended. The flask shell provides all the tools needed to let coders access the functions the exact same way the website does, except in a shell environment. Run the Shell Open up a terminal at the flagging folder. Activate a Python virtual environment: python3 -m venv venv source venv/bin/activate python3 -m pip install -r requirements.txt Set up the FLASK_ENV environment variable: export FLASK_ENV = development Run the shell: flask shell And you should be good to go! The functions listed below should be available for use, and the section below contains some example use cases for the shell. Tip To exit from the shell, type exit() then Enter . Available Shell Functions and Variables app ( flask . Flask ) : The actual Flask app instance. db flask_sqlalchemy . SQLAlchemy : The object used to interact with the Postgres database. get_live_hobolink_data ( (Optional[str]) -> pd.DataFrame ): Gets the HOBOlink data table based on the given \"export\" name. get_live_usgs_data ( () -> pd.DataFrame ): Gets the USGS data table. get_data ( () -> pd.DataFrame ): Gets the Hobolink and USGS data tables and returns a combined table. process_data ( (pd.DataFrame, pd.DataFrame) -> pd.DataFrame ): Combines the Hobolink and USGS tables. compose_tweet ( () -> str ): Generates a message for Twitter that represents the current status of the flagging program (note: this function does not actually send the Tweet to Twitter.com). Additionally, Pandas and Numpy are already pre-imported via import pandas as pd and import numpy as np . Tip To add more functions and variables that pre-load in the Flask shell, simply add another entry to the dictionary returned by the function make_shell_context() in flagging_site/app.py:creat_app() . Tip All of the website's functions can be run in the Flask shell, even those that are not pre-loaded in the shell's global context. All you need to do is import it. For example, let's say you want to get the un-parsed request object from USGS.gov. You can import the function we use and run it like this: # (in Flask shell) from flagging_site.data.usgs import request_to_usgs res = request_to_usgs () print ( res . json ()) Example 1: Export Hobolink Data to CSV Here we assume you have already started the Flask shell. This example shows how to download the Hobolink data and save it as a CSV file. # (in Flask shell) hobolink_data = get_live_hobolink_data () hobolink_data . to_csv ( 'path/where/to/save/my-CSV-file.csv' ) Downloading the data may be useful if you want to see Example 2: Preview Tweet Let's say you want to preview a Tweet that would be sent out without actually sending it. The compose_tweet() function returns a string of this message: # (in Flask shell) print ( compose_tweet ())","title":"Shell"},{"location":"development_resources/shell/#flask-shell-documentation","text":"The shell is used to access app functions and data, such as Hobolink and USGS data and access to the database. The reason why the shell is useful is because there may be cases where you want to play around with the app's functions. For example, maybe you see something that seems fishy in the data, so you want to have direct access to the function the website is running. You may also want to The way Flask works makes it impossible to run the website's functions outside the Flask app context, which means importing the functions into a naked shell doesn't work as intended. The flask shell provides all the tools needed to let coders access the functions the exact same way the website does, except in a shell environment.","title":"Flask Shell Documentation"},{"location":"development_resources/shell/#run-the-shell","text":"Open up a terminal at the flagging folder. Activate a Python virtual environment: python3 -m venv venv source venv/bin/activate python3 -m pip install -r requirements.txt Set up the FLASK_ENV environment variable: export FLASK_ENV = development Run the shell: flask shell And you should be good to go! The functions listed below should be available for use, and the section below contains some example use cases for the shell. Tip To exit from the shell, type exit() then Enter .","title":"Run the Shell"},{"location":"development_resources/shell/#available-shell-functions-and-variables","text":"app ( flask . Flask ) : The actual Flask app instance. db flask_sqlalchemy . SQLAlchemy : The object used to interact with the Postgres database. get_live_hobolink_data ( (Optional[str]) -> pd.DataFrame ): Gets the HOBOlink data table based on the given \"export\" name. get_live_usgs_data ( () -> pd.DataFrame ): Gets the USGS data table. get_data ( () -> pd.DataFrame ): Gets the Hobolink and USGS data tables and returns a combined table. process_data ( (pd.DataFrame, pd.DataFrame) -> pd.DataFrame ): Combines the Hobolink and USGS tables. compose_tweet ( () -> str ): Generates a message for Twitter that represents the current status of the flagging program (note: this function does not actually send the Tweet to Twitter.com). Additionally, Pandas and Numpy are already pre-imported via import pandas as pd and import numpy as np . Tip To add more functions and variables that pre-load in the Flask shell, simply add another entry to the dictionary returned by the function make_shell_context() in flagging_site/app.py:creat_app() . Tip All of the website's functions can be run in the Flask shell, even those that are not pre-loaded in the shell's global context. All you need to do is import it. For example, let's say you want to get the un-parsed request object from USGS.gov. You can import the function we use and run it like this: # (in Flask shell) from flagging_site.data.usgs import request_to_usgs res = request_to_usgs () print ( res . json ())","title":"Available Shell Functions and Variables"},{"location":"development_resources/shell/#example-1-export-hobolink-data-to-csv","text":"Here we assume you have already started the Flask shell. This example shows how to download the Hobolink data and save it as a CSV file. # (in Flask shell) hobolink_data = get_live_hobolink_data () hobolink_data . to_csv ( 'path/where/to/save/my-CSV-file.csv' ) Downloading the data may be useful if you want to see","title":"Example 1: Export Hobolink Data to CSV"},{"location":"development_resources/shell/#example-2-preview-tweet","text":"Let's say you want to preview a Tweet that would be sent out without actually sending it. The compose_tweet() function returns a string of this message: # (in Flask shell) print ( compose_tweet ())","title":"Example 2: Preview Tweet"}]}